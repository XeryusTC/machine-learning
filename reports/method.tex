To learn how to play Hare and Hounds we will use a reinforcement learning
technique called Q-learning\cite{watkins1992q}.
Each player will have his own different
Q-table to store all values. This means that the actions of the hare are
separate from those of the three hounds combined. We did not opt for using
a separate Q-table for each hound, because the hounds are supposed to be
working together as they are the pieces of a single player. Furthermore,
the states are indifferent to which hound is in which spot. Our state space
consists of all different positions of the hounds and the hare, where all
permutations of hounds are considered the same state.

Each player also has their own set of actions $A$. At each turn, the possible
actions are generated for the player. To select the most appropriate action we
use  the softmax function with a temperature parameter. This means that during
training we can move from exploration to exploitation smoothly. When the
temperature is infinite then pure exploration is used while it would ensure
pure exploitation when its value is very close to zero. The action selection
function looks as follows\cite{alpaydin}
\[ P(a|s) = \frac{\exp[Q(s,a)/T]}{\sum_{b \in A} \exp[Q(s, b)/T]} \]
where $P(a|s)$ is the probability that action $a$ is selected given the
current state $s$, $A$ is the set of available actions in a state, $Q(s,a)$ is
the value from the Q-table for a state-action pair $(s,a)$ and $T$ is the
temperature parameter. At the start of training, the temperature is equal to
the number of games that are played. During training we linearly decrease the
temperature by a single degree after every game, the final temperature in the
last game is one degree.

We train both players by letting them play against each other while they have
the same parameters. The parameters are the learning rate $\eta$ and the
discount factor $\gamma$. The discount factor is constant during training but
$\eta$ is gradually lowered to improve conversion to the optimal
Q-values\cite{alpaydin}. We lower the value of $\eta$ by a constant amount so
that it equal to zero in the last round of training. We train using the
Q-learning algorithm that is shown in Algorithm \autoref{alg:Qlearning}.

A single round of training consists of both players playing the game until one
of them wins or they both have played 50 turns but neither has won yet. We call
this a draw and both players receive a small negative reward. Each player uses
the current $T, \eta, \gamma$ parameters. The Q-table gets updated after each
the opponents so that we do not have to remember the moves that we have but we
do only update the states that have been visited since the outcome of an action
is non-deterministic. Updating the Q-table after the opponents turn also
ensures that a player learns how the opponent reacts to a move. After the
training game is finished then the values for $T$ and $\eta$ are updated and
we start the next training game.

\begin{algorithm}
\caption{Q-learning algorithm, modified from \cite{alpaydin}}
\label{alg:Qlearning}
\begin{algorithmic}[1]
\State{Initialize all $Q(s, a)$ to 0}
\ForAll{episodes}
	\State{Initialize $s$}
	\Repeat
		\State{Choose $a$ using policy derived from $Q$ with softmax}
		\State{Take action $a$, observe $r$ and $s^\prime$}
		\State{$Q(s,a)\gets Q(s,a) + \eta(r + \gamma\max_{a^\prime}
			Q(s^\prime,a^\prime) - Q(s,a))$}
		\State{$s\gets s^\prime$}
	\Until{$s$ is terminal}
        \State{Lower $\eta$ and $T$}
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Experimental design}
To test the implementation we trained the system for with different
configurations of the parameters $\eta$ and $\gamma$ as described above. We
also have a parameter which we call \emph{runs} which is the number of games
that are played during training. We use a varying number of \emph{runs}
because we do not know how many training games are needed for the Q-values to
converge. At the same time this can also be used to check on training progress.
The set of configurations can be seen in \autoref{tbl:conf}, we train on each
permutation of the three settings to see which parameter settings lead to the
best results. We use a wider range of settings than is conventional so that we
can see what the effects of more extreme values are.

We obtain results by letting the trained players play against each other. We
keep track of how often each player wins over a number of games, the
proportion of games that a player wins is used as the performance measure. Hare
and Hounds is slightly biased towards the hounds player, this means that we can
expect most of the games to be won by the hounds player if the players have
been trained successfully.

\begin{table}
    \centering
    \caption{Different values of the parameters that were used for training.
    	We used all possible combinations of the three parameters for training
    	and compare the resulting models.}
    \label{tbl:conf}
    \begin{tabular}{l|rrrrrrrr}
        \hline
        parameter & \\
        \hline
        runs & 100 & 1000 & 10000 & 100000 & 5000000 \\
        discount factor $\gamma$ & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & 0.95 &
        0.99 \\
        learning rate $\eta$ & 0.01 & 0.05 & 0.1 & 0.15 & 0.2 & 0.25 & 0.3
        & 0.5 \\
        \hline
    \end{tabular}
\end{table}

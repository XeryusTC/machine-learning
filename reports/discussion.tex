We can see that after enough training the hounds will always win for most of
parameter settings. \autoref{fig:r5000000} shows a horizontal line at 1.0 for
all parameters except when $\gamma = 0.5$. This means that the hounds player
always wins for each configuration where $\gamma \geq 0.6$. We can thus say
that these settings are optimal for the hounds player.

From \autoref{fig:dat} we can see that we need about 100 000 games for training
to converge. We can from \autoref{fig:r10000} that 10 000 games are not enough
since some combinations of the different parameter configurations are not fully
horizontal yet, but that most of the configurations do end up in a state where
hounds always wins. The difference between 100 000 and 5 000 000 games is not
very large, we can even say that 5 000 000 games is slightly worse for the
hounds player but better for the hare player since some points are more
beneficial to the hare player. However this is not a very large difference and
thus we can conclude that it is not worth the extra training time.

Although the game does not have a very large state space because the board is
so small, it might still take up a large amount of memory to store it. We
decreased the size of the state space by not considering the hounds to be
unique. If the position of two hounds is swapped then the state remains the
same. If we did consider the hounds to be unique then the state would also
change when we swap the position of two hounds. Another way to decrease the
size of the state space is by exploiting the symmetry of the board. The board
is symmetrical horizontally so this means that we could consider states to be
equal when they are horizontally flipped versions of each other. Removing these
flipped versions would mean that the state space is also decreased. This might
also improve learning since there are far fewer states to learn in this case.

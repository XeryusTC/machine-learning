We can see that after enough training the hounds will always win for most of
parameter settings. \autoref{fig:r5000000} shows a horizontal line at 1.0 for
all parameters except when $\gamma = 0.5$. This means that the hounds player
always wins for each configuration where $\gamma \geq 0.6$. We can thus say
that these settings are optimal for the hounds player.

From \autoref{fig:dat} we can see that we need about 100 000 games for training
to converge. In \autoref{fig:r10000} we can see that 10 000 games are not enough
since some combinations of the different parameter configurations are not fully
horizontal yet, but that most of the configurations do end up in a state where
hounds always wins. The difference between 100 000 and 5 000 000 games is not
very large, we can even say that 5 000 000 games is slightly worse for the
hounds player but better for the hare player since some points are more
beneficial to the hare player. However this is not a very large difference and
thus we can conclude that it is not worth the extra training time.

Although the game does not have a very large state space because the board is
so small, it might still take up a large amount of memory to store it. We
decreased the size of the state space by not considering the hounds to be
unique. If the position of two hounds is swapped then the state remains the
same. If we did consider the hounds to be unique then the state would also
change when we swap the position of two hounds. Another way to decrease the
size of the state space is by exploiting the symmetry of the board. The board
is symmetrical horizontally so this means that we could consider states to be
equal when they are horizontally flipped versions of each other. Removing these
flipped versions would mean that the state space is also decreased. This might
also improve learning since there are far fewer states to learn in this case.

Another option to reduce storage complexity is to use one single table for both
players. In this case the hare player would select an action that would lead
to the outcome with the lowest reward, i.e. the largest negative value. This
should not lead to any problems with overlapping actions since the sets of
actions are disjoint. Our implementation models the possible actions of the
hounds player as moving a piece from one place to another while the set of
possible actions of the hare player is moving to a square. The difference
may be subtle but it means that one player could not possibly accidentally
pick the action of another player, nor can they update each other's Q-values.

Although we only used softmax to select the action that players should perform
we could also have opted for $\varepsilon$-greedy where the option to take a
random action is picked with probability $\varepsilon$, this would be the
proportion we want to explore. On the other hand the proportion of exploitation
is $1-\varepsilon$, in that case we pick the best known action. It could be
interesting to see the effect of using the softmax vs. the $\varepsilon$-greedy
method would be.

It seems that using reinforcement learning to learn to play the game Hound and
Hare works well. Using Q-learning gives us the results we would expect in
a decent about of training epochs. There are still interesting things that
could be done using this technique. In our study, we only compared the
performance of the hounds against a hare which was trained with the same
parameters. An interesting thing to test would be to compare a set of
hounds with different training parameters from the hare, so both players
might have had different discount factors during training, leading to
different kinds of behaviors.

Another interesting line of research would be to compare the learned policy
from Q-learning with the policies found in combinatorial game theory
\cite{gardner1961second, siegel2005coping}. If these policies are the same,
then Q-learning does indeed learn the optimal policy, but if they are
different the reinforcement learning method might not have searched through
the state space sufficiently enough. On the other hand, Q-learning might
also find better policy than the combinatorial game theorists have found,
sparking new research in that area.
